Coursera : Structuring Machine Learning Projects (Lecturer : Andrew Ng ğŸ¥‡ )

### 1. ML Modelì˜ ì˜ˆì¸¡ ì •í™•ë„ë¥¼ ë” ë†’ì´ê³  ì‹¶ì€ ê²½ìš°ì˜ ì„ íƒì§€

1. Collect more data
2. Collect more diverse training set
3. Train algorithm longer with gradient descent
4. Try Adam instead of gradient descent
5. Try bigger network
6. Try smaller network
7. Try dropout
8. Add L_2 regularization
9. Change Network Architecture
10. Change Activation Func.
11. Change # of hidden units

### 2. ì´ ì¤‘ ë¬´ì—‡ì´ ë§ëŠ” approachì¸ì§€, ë¹ ë¥´ê²Œ ì•Œ ìˆ˜ëŠ” ì—†ì„ê¹Œ??

- ML ì‹œìŠ¤í…œ êµ¬ì¶• ì‹œì˜ Challenge : ì‹œë„í•  ìˆ˜ ìˆëŠ” ë°©ë²•, ë°”ê¿€ ìˆ˜ ìˆëŠ” Hyperparam. ë“±ì´ ë¬´ìˆ˜íˆ ë§ë‹¤.
- MLì˜ ëŒ€ê°€ë“¤ì€,,, Aë¥¼ ë³€í™”ì‹œí‚¤ê¸° ìœ„í•´ Bë¥¼ Tune ì‹œì¼œì•¼ í•˜ëŠ” ê²ƒì„ ì•ˆë‹¤!

### 3. Orthogonalization (Matrix Theoryì—ì„œì˜ Orthogonalizationê³¼ëŠ” ë‹¤ë¦„)

- ì§ê´€ì  ì˜ˆì‹œ : TVí™”ë©´ì„ ì¡°ì •í•˜ê¸° ìœ„í•œ ì—¬ëŸ¬ê°œì˜ ë…¸ë¸Œë“¤ì´ ê°ê°ì˜ ê³ ìœ í•œ í•˜ë‚˜ì˜ ê¸°ëŠ¥ë§Œì„ ìˆ˜í–‰í•˜ë„ë¡ í•˜ëŠ” ê²ƒ.
- í•˜ë‚˜ì˜ ì†ì¡ì´ê°€ ì—¬ëŸ¬ ì¸¡ë„ë¥¼ í•œêº¼ë²ˆì— ë³€í™”ì‹œí‚¤ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, ê°€ë¡œê¸¸ì´ë§Œì„ ë³€í™”ì‹œí‚¤ëŠ” ë“±,,,

### 4. Chain of Assumptions in ML

1. Fit training set well on cost function (íŠ¹ì • ë¶„ì•¼ì—ì„œëŠ”, ì¸ê°„ê³¼ ìœ ì‚¬í•œ ìˆ˜ì¤€ì„ ì˜ë¯¸í•¨)

##### ë§Œì•½ Learning Alg. Train ê³¼ì •ì—ì„œ metric (accuracy, F1-score ë“±)ì´ ì˜ ì•ˆ ë‚˜ì˜¨ë‹¤ë©´ ì“°ëŠ” ë°©ë²•ë¡ 

      - Train Bigger Network
      - Adam Optimizer

2. Fit Dev set well on cost function

cf. (Dev set = Holdout Cross Validation Set, Hyperparam. Tuningì„ ìœ„í•´ ì‚¬ìš©í•˜ëŠ” Sets)

###### Validation ê³¼ì •ì—ì„œ metricì´ ì˜ ì•ˆë‚˜ì˜¨ë‹¤ë©´ ì“°ëŠ” ë°©ë²•ë¡ 

      - Regularization
      - Bigger Training Set

3. Fit test set well on cost function

##### Test Setì—ì„œ Alg.ì´ ì˜ Work í•˜ì§€ ì•ŠëŠ”ë‹¤ë©´?

      - Bigger Dev. Set. (âˆµDev. set ì—ì„œ Overtraining ì´ ì´ë£¨ì–´ì¡Œì„ ìˆ˜ ìˆê¸° ë•Œë¬¸.)

4. Fit well real word

###### ì‹¤ì œ datasetì—ì„œì˜ inference ì •í™•ë„ê°€ ì¢‹ì§€ ì•Šë‹¤ë©´?

      - Change Dev. set (Dev./Test set distributionì´ ì œëŒ€ë¡œ ë˜ì§€ ì•Šì€ ê²½ìš°)
      - Change Cost func. (Cost func. isnâ€™t measuring the right thing)

### 5. Single number evaluation metrics (to choose hyperparams.)

1.  Precision : ì •ë‹µì„ ì‹¤ì œë¡œ ë§ì¶˜ ê°œìˆ˜ / ì •ë‹µìœ¼ë¡œ ì°ì€ ê°œìˆ˜
    => Aë¼ê³  Guess í–ˆì„ ë•Œ, ì‹¤ì œë¡œ Aì¼ í™•ë¥  = Precision
2.  Recall : ì •ë‹µì„ ì‹¤ì œë¡œ ë§ì¶˜ ê°œìˆ˜ / ì‹¤ì œ ì •ë‹µ ê°œìˆ˜
    => ì‹¤ì œ ì •ë‹µ ì¤‘ ëª‡ %ê°€ ì •ë‹µìœ¼ë¡œ ë¶„ë¥˜ë˜ì—ˆëŠ”ê°€?
    cf. Precision & Recall ì€ ì¼ë°˜ì ìœ¼ë¡œ TradeOff ê´€ê³„
    => Classifer AëŠ” Precisionì—ì„œ ë” ë†’ê³ , Classifer BëŠ” Recallì—ì„œ ë” ë†’ë‹¤ë©´,, ì• ë§¤í•œ ìƒí™©ì´ ë¨.
    => ì´ ë‘ê°œë§Œìœ¼ë¡œëŠ”, ì–´ë–¤ hyperparamìœ¼ë¡œ ë¹ ë¥´ê²Œ ì„ íƒí•´ì„œ loop ëŒë ¤ì•¼í• ì§€ ì•Œ ìˆ˜ ì—†ë‹¤.
3.  Pì™€ Rì˜ ì¡°í™”í‰ê· 

![weights](./img/struct_ml_pjt_01_f1_score.png)

    - Reasonable way to combine R and P
    - ì˜ ì •ì˜ëœ Dev. Set ë„ ì¤‘ìš”, (P and Rì„ ì¸¡ì •í•˜ê¸° ìœ„í•´ Dev. Setì„ ì‚¬ìš©)

### 6. Optimizing metric (to combine F1 Score & Running time)

1. E.G.> Wants to Choose a classifier that maximizes Accuracy (F1 Score), but subject to running time <= 100 ms
2. ì—¬ê¸°ì„œ Optimizing metric = Accuracy, Satisficing metric = Running time

### 7. Train / Dev / Test Distribution

- Choose a Dev set and Test set that reflect data that you expect to get in future
- Dev Setê³¼ Test Setì´ ë™ì¼í•œ Distributionì„ ê°€ì§€ë„ë¡

1. Old way (Data less than 10,000 rows)
   | Train | Test |
   | -------------- |------|
   | -------70%------- | ---30%---|

   | Train           | Val     | Test    |
   | --------------- | ------- | ------- |
   | ------60%------ | --20%-- | --20%-- |

2. Contemporary way (Data about 1,000,000 rows)

   | Train                                                                              | Val  | Test |
   | ---------------------------------------------------------------------------------- | ---- | ---- |
   | ------------------------------------------98%------------------------------------- | -1%- | -1%- |

3. When to change Dev/Test set and metric

- í˜„ì¬ì˜ Evaluation Metricì´ ì‚¬ìš©ì Preferenceë¥¼ ì œëŒ€ë¡œ ë°˜ì˜í•˜ì§€ ëª»í•˜ê³  ìˆëŠ” ê²½ìš°

### 8. Error

1. e.g. Alg A ì˜ ErrorëŠ” 3%, Alg Bì˜ ErrorëŠ” 5% ì§€ë§Œ Aì˜ Errorê°€ ì¤‘ëŒ€í•œ ì‹¤ìˆ˜(pornographic imageë¥¼ catìœ¼ë¡œ ë¶„ë¥˜)ë¥¼ ì €ì§€ë¥´ëŠ” ê²½ìš°ì—ëŠ” ì–´ë–»ê²Œ í•´ê²°í•  ìˆ˜ ìˆì„ê¹Œ?
   => ê°€ì¤‘ì¹˜ë¡œ í•´ê²°

![weights](./img/struct_ml_pjt_01_w.png)

- ì›ë˜ ë¶„ëª¨ëŠ” devì˜ ê°œìˆ˜ì´ì§€ë§Œ ê°€ì¤‘ì¹˜ê°€ ì¶”ê°€ë˜ë©´ì„œ normalizing constantê°€ ë°”ë€œ.
- Defining Evaluation metric (Targetì„ place ì‹œí‚¤ëŠ” ê²ƒ) ê³¼ Do well on some metric (Targetì— ì˜ ë§ì¶”ëŠ” ê²ƒ) ì€ ë³„ê°œì˜ ë¬¸ì œì„.
  - Orthogonalizationì˜ Concept,,, ë‘ ê°œì˜ ë³„ë„ nobsë¥¼ ê°€ì§€ê³  working

1. Bayes Optimal Error : Best possible error

### 9. ML Algê°€ Human levelë³´ë‹¤ ëª»í•˜ê³  ìˆëŠ” ê²½ìš° ì“¸ ìˆ˜ ìˆëŠ” technics

1. Get labeled data from humans
2. Gain insights from manual error analysis : ì‚¬ëŒì€ ì–´ë–»ê²Œ ë§ì¶œ ìˆ˜ ìˆëŠ”ê°€?
3. Better analysis of bias / variance

- Human levelê³¼ Train setì—ì„œì˜ Alg performanceë¥¼ ë¹„êµí•˜ëŠ” ê²ƒì€ ì¤‘ìš”.
- Why? Alg.ì„ ì–´ë–»ê²Œ ê°œì„ í•´ì•¼ í•  ì§€ íŒíŠ¸ë¥¼ ì–»ì„ ìˆ˜ ìˆìŒ.
  => 'Human Errorì€ Bayes Errorì— ëŒ€í•œ Proxy(ëŒ€ë¦¬ì¸)' ì´ë¼ëŠ” í‘œí˜„ì„ ì”€.

### 10. Bias vs Variance ì˜ˆì œ

- bias-variance tradeoff (Link : https://ko.wikipedia.org/wiki/%ED%8E%B8%ED%96%A5-%EB%B6%84%EC%82%B0_%ED%8A%B8%EB%A0%88%EC%9D%B4%EB%93%9C%EC%98%A4%ED%94%84)
- ì˜ˆì œ
  | Error ìœ í˜• | ì˜ˆì œ1 | ì˜ˆì œ2 |
  | -----|------|-----|
  | Human Error | 1% | 7.5% |
  | Train Error | 8% | 8% |
  | Dev Error | 10% | 10% |

- ì˜ˆì œ 1 Solution : Biasì— ì§‘ì¤‘
  - Algì´ Train setì— Fit ì˜ ë˜ì§€ X
  - Bigger NN
  - Train longer
  - Use better optimization alg (RMS Prop, Adam, â€¦)
  - Find better NN architecture, hyperparam (Change activation fuc/ # of layers / # of hidden units,,, / RNNì´ë‚˜ CNNë“± ì‚¬ìš© /....)
- ì˜ˆì œ 2 Solution : Varianceì— ì§‘ì¤‘

  - Regularization (L_2 Regularization / Dropout / Data Augmentation)
  - Getting more training data
  - Find better NN architecture, hyperparam

- Avoidable Bias = Gap between Bayes Error & Train Error
  - Bayes Error ë³´ë‹¤ ì˜í•œë‹¤ë©´? Overfitting ì˜ˆìƒ.
- Variance = Gap between Train Error & Dev Error

### 11. ML ì´ ì‚¬ëŒë³´ë‹¤ ì˜ í•˜ê³  ìˆëŠ” ë¶„ì•¼ë“¤

1. Online Advertising
2. Product Recommendations
3. Logistics (ë¬¼ë¥˜, transit time ì˜ˆì¸¡)
4. Loan approvals

- ìœ„ ë¶„ì•¼ë“¤ ê³µí†µì 
  1.  Structured Data ì¡´ì¬
  2.  Natural perception (image recognition ë“±,,,) ë“± ì˜ ì‚¬ëŒì´ ì˜ í•˜ëŠ” ë¶„ì•¼ê°€ ì•„ë‹˜.
  3.  DataëŸ‰ ë§ìŒ.
