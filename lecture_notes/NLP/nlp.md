### Category 1. Architectures
- Traditional ML : "bag of words" => No order
- RNN : for sequences => Maintain order
- RNN 변형 : LSTM, GRU, ... 

### Category 2. Embeddings
- Word2Vec, GloVe
- Gives words a location
- models relationships between words
- Word similarities 
- Analogies (King - man = Queen - woman)


Machine Translation -> Seq2Seq model 보통 사용 (input seq과 output seq. 길이 달라도 ok.)

